{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13112fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "1. **Understand** the Frozen Lake environment and reinforcement learning basics\n",
    "2. **Implement** a Deep Q-Network (DQN) from scratch using PyTorch\n",
    "3. **Train** the neural network to solve the Frozen Lake problem\n",
    "4. **Visualize** the learning process and agent's performance\n",
    "5. **Analyze** the trained policy and Q-values\n",
    "\n",
    "Frozen Lake is a classic reinforcement learning environment where:\n",
    "- **Goal**: Navigate from start (S) to goal (G) on a frozen lake\n",
    "- **Challenge**: Avoid falling into holes (H) while walking on frozen tiles (F)\n",
    "- **Complexity**: The lake is slippery - you don't always move in the intended direction!\n",
    "\n",
    "```\n",
    "SFFF    S = Start\n",
    "FHFH    F = Frozen (safe)\n",
    "FFFH    H = Hole (game over)\n",
    "HFFG    G = Goal (success!)\n",
    "```\n",
    "\n",
    "\n",
    "Q-Learning learns the **Q-function**: Q(s,a) = expected future reward when taking action 'a' in state 's'.\n",
    "\n",
    "- **Traditional Q-Learning**: Uses a table to store Q-values\n",
    "- **Deep Q-Learning**: Uses a neural network to approximate the Q-function\n",
    "- **Advantage**: Can handle large or continuous state spaces\n",
    "\n",
    "1. **Neural Network**: Approximates Q(s,a) for all actions given a state\n",
    "2. **Experience Replay**: Stores and randomly samples past experiences\n",
    "3. **Target Network**: Stabilizes training\n",
    "4. **Œµ-greedy Policy**: Balances exploration vs exploitation\n",
    "\n",
    "```\n",
    "Q(s,a) = r + Œ≥ * max(Q(s',a'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install gymnasium[toy-text] torch torchvision matplotlib seaborn numpy pandas tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All packages installed and imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üèãÔ∏è Gymnasium version: {gym.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b352a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='rgb_array')\n",
    "\n",
    "print(\"üèîÔ∏è Frozen Lake Environment Information:\")\n",
    "print(f\"üìä Observation Space: {env.observation_space}\")\n",
    "print(f\"üéÆ Action Space: {env.action_space}\")\n",
    "print(f\"üó∫Ô∏è Map Size: 4x4 = 16 states\")\n",
    "print(f\"üéØ Actions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Map Layout:\")\n",
    "desc = env.unwrapped.desc\n",
    "for i, row in enumerate(desc):\n",
    "    if hasattr(row[0], 'decode'):\n",
    "        row_str = ' '.join([cell.decode('utf-8') for cell in row])\n",
    "    else:\n",
    "        row_str = ' '.join([str(cell) for cell in row])\n",
    "    print(f\"Row {i}: {row_str}\")\n",
    "\n",
    "state, info = env.reset()\n",
    "print(f\"\\nüöÄ Initial state: {state}\")\n",
    "print(f\"üìç Initial position: Row {state // 4}, Column {state % 4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3333e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_environment(env, state=None, title=\"Frozen Lake Environment\"):\n",
    "    \"\"\"Visualize the Frozen Lake environment with the agent's current position.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    \n",
    "    desc = env.unwrapped.desc\n",
    "    nrows, ncols = desc.shape\n",
    "    \n",
    "    colors = {\n",
    "        b'S': 'lightgreen',  # Start\n",
    "        b'F': 'lightblue',   # Frozen\n",
    "        b'H': 'red',         # Hole\n",
    "        b'G': 'gold'         # Goal\n",
    "    }\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            tile = desc[i, j]\n",
    "            color = colors[tile]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrows-1-i), 1, 1, \n",
    "                               facecolor=color, edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            ax.text(j+0.5, nrows-1-i+0.5, tile.decode('utf-8') if hasattr(tile, 'decode') else str(tile), \n",
    "                   ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    if state is not None:\n",
    "        agent_row = state // ncols\n",
    "        agent_col = state % ncols\n",
    "        \n",
    "        circle = plt.Circle((agent_col+0.5, nrows-1-agent_row+0.5), 0.3, \n",
    "                          color='purple', alpha=0.8, zorder=10)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        ax.text(agent_col+0.5, nrows-1-agent_row+0.5, 'ü§ñ', \n",
    "               ha='center', va='center', fontsize=16, zorder=11)\n",
    "    \n",
    "    ax.set_xlim(0, ncols)\n",
    "    ax.set_ylim(0, nrows)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='Start (S)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', label='Frozen (F)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='red', label='Hole (H)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='gold', label='Goal (G)'),\n",
    "        plt.Circle((0, 0), 0.1, color='purple', label='Agent ü§ñ')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_environment(env, state, \"üèîÔ∏è Frozen Lake - Initial State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Frozen Lake environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=16, action_size=4, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1 and x.dtype == torch.long:\n",
    "            x = F.one_hot(x, num_classes=16).float()\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "dqn = DQN().to(device)\n",
    "print(f\"\\nüß† DQN Architecture:\")\n",
    "print(dqn)\n",
    "\n",
    "test_state = torch.tensor([0], dtype=torch.long).to(device)\n",
    "test_output = dqn(test_state)\n",
    "print(f\"\\nüß™ Test input (state 0): {test_state}\")\n",
    "print(f\"üéØ Test output (Q-values): {test_output}\")\n",
    "print(f\"üìä Network parameters: {sum(p.numel() for p in dqn.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40116682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for storing and sampling experiences.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.tensor([e.state for e in experiences], dtype=torch.long).to(device)\n",
    "        actions = torch.tensor([e.action for e in experiences], dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor([e.next_state for e in experiences], dtype=torch.long).to(device)\n",
    "        dones = torch.tensor([e.done for e in experiences], dtype=torch.bool).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"üì¶ Replay Buffer implemented successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8409819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network Agent for solving Frozen Lake.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=16, action_size=4, lr=0.001, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=32, target_update=100):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        self.q_network = DQN(state_size, action_size).to(device)\n",
    "        self.target_network = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.training_step = 0\n",
    "        self.losses = []\n",
    "        \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Choose an action using Œµ-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state_tensor = torch.tensor([state], dtype=torch.long).to(device)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer.\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the network on a batch of experiences.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Get Q-values for a given state.\"\"\"\n",
    "        state_tensor = torch.tensor([state], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            return self.q_network(state_tensor).cpu().numpy()[0]\n",
    "\n",
    "agent = DQNAgent()\n",
    "print(\"ü§ñ DQN Agent created successfully!\")\n",
    "print(f\"üß† Network parameters: {sum(p.numel() for p in agent.q_network.parameters())}\")\n",
    "print(f\"üéØ Initial epsilon (exploration rate): {agent.epsilon:.3f}\")\n",
    "print(f\"üíæ Replay buffer capacity: {agent.memory.capacity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_dqn(agent, env, episodes=2000, max_steps=100, print_every=200):\n",
    "    \"\"\"Train the DQN agent.\"\"\"\n",
    "    scores = []\n",
    "    success_rate = []\n",
    "    epsilon_history = []\n",
    "    recent_scores = deque(maxlen=100)\n",
    "    \n",
    "    print(\"üöÄ Starting DQN Training...\")\n",
    "    print(f\"üìä Episodes: {episodes}, Max steps per episode: {max_steps}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for episode in tqdm(range(episodes), desc=\"Training Progress\"):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.act(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        recent_scores.append(total_reward)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        if len(recent_scores) >= 100:\n",
    "            success_rate.append(sum(recent_scores) / len(recent_scores))\n",
    "        else:\n",
    "            success_rate.append(sum(recent_scores) / len(recent_scores))\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_score = np.mean(recent_scores)\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Score: {avg_score:.3f} | Success Rate: {success_rate[-1]:.3f} | Epsilon: {agent.epsilon:.3f} | Buffer Size: {len(agent.memory)}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "    \n",
    "    return {\n",
    "        'scores': scores,\n",
    "        'success_rate': success_rate,\n",
    "        'epsilon_history': epsilon_history,\n",
    "        'losses': agent.losses\n",
    "    }\n",
    "\n",
    "training_stats = train_dqn(agent, env, episodes=2000, max_steps=100, print_every=200)\n",
    "\n",
    "print(f\"\\nüéâ Final training statistics:\")\n",
    "print(f\"üìà Final success rate: {training_stats['success_rate'][-1]:.3f}\")\n",
    "print(f\"üéØ Final epsilon: {training_stats['epsilon_history'][-1]:.3f}\")\n",
    "print(f\"üíæ Total experiences collected: {len(agent.memory)}\")\n",
    "print(f\"üîÑ Training steps completed: {agent.training_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78898015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_results(stats):\n",
    "    \"\"\"Plot comprehensive training results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('üß† DQN Training Results - Frozen Lake', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes[0, 0].plot(stats['success_rate'], color='green', linewidth=2)\n",
    "    axes[0, 0].set_title('üìà Success Rate Over Time')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Success Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Target (80%)')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].plot(stats['epsilon_history'], color='orange', linewidth=2)\n",
    "    axes[0, 1].set_title('üéØ Exploration Rate (Epsilon) Decay')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Epsilon')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    window_size = 50\n",
    "    if len(stats['scores']) >= window_size:\n",
    "        smoothed_scores = pd.Series(stats['scores']).rolling(window=window_size).mean()\n",
    "        axes[1, 0].plot(smoothed_scores, color='blue', linewidth=2, label=f'Smoothed (window={window_size})')\n",
    "    \n",
    "    axes[1, 0].plot(stats['scores'], color='lightblue', alpha=0.3, label='Raw scores')\n",
    "    axes[1, 0].set_title('üèÜ Episode Scores')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    if stats['losses']:\n",
    "        loss_window = min(100, len(stats['losses']) // 10)\n",
    "        if len(stats['losses']) >= loss_window:\n",
    "            smoothed_loss = pd.Series(stats['losses']).rolling(window=loss_window).mean()\n",
    "            axes[1, 1].plot(smoothed_loss, color='red', linewidth=2)\n",
    "        \n",
    "        axes[1, 1].set_title('üìâ Training Loss')\n",
    "        axes[1, 1].set_xlabel('Training Step')\n",
    "        axes[1, 1].set_ylabel('MSE Loss')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Training Summary:\")\n",
    "    print(f\"üéØ Final Success Rate: {stats['success_rate'][-1]:.1%}\")\n",
    "    print(f\"üìà Best Success Rate: {max(stats['success_rate']):.1%}\")\n",
    "    print(f\"üèÜ Total Successful Episodes: {sum(stats['scores'])}\")\n",
    "    print(f\"üìâ Final Exploration Rate: {stats['epsilon_history'][-1]:.3f}\")\n",
    "    \n",
    "    if stats['losses']:\n",
    "        print(f\"üî• Final Training Loss: {stats['losses'][-1]:.6f}\")\n",
    "        print(f\"üìä Average Training Loss: {np.mean(stats['losses']):.6f}\")\n",
    "\n",
    "plot_training_results(training_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_agent(agent, env, num_episodes=100):\n",
    "    \"\"\"Test the trained agent's performance.\"\"\"\n",
    "    test_scores = []\n",
    "    test_steps = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    print(f\"üß™ Testing agent for {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            action = agent.act(state, training=False)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        test_scores.append(total_reward)\n",
    "        test_steps.append(steps)\n",
    "        \n",
    "        if total_reward > 0:\n",
    "            successful_episodes += 1\n",
    "    \n",
    "    success_rate = successful_episodes / num_episodes\n",
    "    avg_score = np.mean(test_scores)\n",
    "    avg_steps = np.mean(test_steps)\n",
    "    \n",
    "    print(f\"\\nüéØ Test Results:\")\n",
    "    print(f\"‚úÖ Success Rate: {success_rate:.1%} ({successful_episodes}/{num_episodes})\")\n",
    "    print(f\"üèÜ Average Score: {avg_score:.3f}\")\n",
    "    print(f\"üë£ Average Steps: {avg_steps:.1f}\")\n",
    "    print(f\"üé≤ Score Distribution: {np.bincount(test_scores)}\")\n",
    "    \n",
    "    return {\n",
    "        'success_rate': success_rate,\n",
    "        'scores': test_scores,\n",
    "        'steps': test_steps,\n",
    "        'successful_episodes': successful_episodes\n",
    "    }\n",
    "\n",
    "test_results = test_agent(agent, env, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbed221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_q_values(agent, env):\n",
    "    \"\"\"Analyze and visualize the learned Q-values.\"\"\"\n",
    "    q_table = np.zeros((16, 4))\n",
    "    \n",
    "    for state in range(16):\n",
    "        q_values = agent.get_q_values(state)\n",
    "        q_table[state] = q_values\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üß† Learned Q-Values Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    sns.heatmap(q_table, annot=True, fmt='.2f', cmap='viridis',\n",
    "                xticklabels=action_names, yticklabels=range(16),\n",
    "                ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Q-Values for All States')\n",
    "    axes[0, 0].set_xlabel('Actions')\n",
    "    axes[0, 0].set_ylabel('States')\n",
    "    \n",
    "    best_actions = np.argmax(q_table, axis=1)\n",
    "    action_grid = best_actions.reshape(4, 4)\n",
    "    \n",
    "    sns.heatmap(action_grid, annot=True, fmt='d', cmap='Set3',\n",
    "                ax=axes[0, 1], cbar_kws={'label': 'Best Action'})\n",
    "    axes[0, 1].set_title('Best Action per State (0=L, 1=D, 2=R, 3=U)')\n",
    "    \n",
    "    axes[1, 0].hist(q_table.flatten(), bins=30, alpha=0.7, color='skyblue')\n",
    "    axes[1, 0].set_title('Q-Value Distribution')\n",
    "    axes[1, 0].set_xlabel('Q-Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    state_values = np.max(q_table, axis=1).reshape(4, 4)\n",
    "    sns.heatmap(state_values, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('State Values (Max Q-Value)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "q_table = analyze_q_values(agent, env)\n",
    "\n",
    "print(\"\\nüéØ Q-Values Analysis:\")\n",
    "print(f\"üìä Q-value range: [{q_table.min():.3f}, {q_table.max():.3f}]\")\n",
    "print(f\"üìà Average Q-value: {q_table.mean():.3f}\")\n",
    "print(f\"üé≤ Q-value std: {q_table.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1cbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_policy(agent, env):\n",
    "    \"\"\"Visualize the learned policy on the grid.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    \n",
    "    desc = env.unwrapped.desc\n",
    "    nrows, ncols = desc.shape\n",
    "    \n",
    "    colors = {\n",
    "        b'S': 'lightgreen',  # Start\n",
    "        b'F': 'lightblue',   # Frozen\n",
    "        b'H': 'red',         # Hole\n",
    "        b'G': 'gold'         # Goal\n",
    "    }\n",
    "    \n",
    "    action_arrows = {\n",
    "        0: '‚Üê',  # Left\n",
    "        1: '‚Üì',  # Down\n",
    "        2: '‚Üí',  # Right\n",
    "        3: '‚Üë'   # Up\n",
    "    }\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            tile = desc[i, j]\n",
    "            color = colors[tile]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrows-1-i), 1, 1, \n",
    "                               facecolor=color, edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            ax.text(j+0.2, nrows-1-i+0.8, tile.decode('utf-8') if hasattr(tile, 'decode') else str(tile), \n",
    "                   ha='left', va='top', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            state = i * ncols + j\n",
    "            if tile != b'H':\n",
    "                best_action = agent.act(state, training=False)\n",
    "                arrow = action_arrows[best_action]\n",
    "                ax.text(j+0.5, nrows-1-i+0.4, arrow, \n",
    "                       ha='center', va='center', fontsize=24, \n",
    "                       color='darkblue', fontweight='bold')\n",
    "                \n",
    "                q_values = agent.get_q_values(state)\n",
    "                max_q = np.max(q_values)\n",
    "                ax.text(j+0.8, nrows-1-i+0.2, f'{max_q:.2f}', \n",
    "                       ha='right', va='bottom', fontsize=10, \n",
    "                       color='darkred', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncols)\n",
    "    ax.set_ylim(0, nrows)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('üß† Learned Policy Visualization\\n(Arrows show best actions, numbers show max Q-values)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='Start (S)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', label='Frozen (F)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='red', label='Hole (H)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='gold', label='Goal (G)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy(agent, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def demonstrate_agent(agent, env, num_demos=5):\n",
    "    \"\"\"Demonstrate the trained agent playing the game.\"\"\"\n",
    "    print(\"üéÆ Agent Demonstration:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    for demo in range(num_demos):\n",
    "        print(f\"\\nüéØ Demo {demo + 1}:\")\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        path = [state]\n",
    "        actions_taken = []\n",
    "        \n",
    "        for step in range(20):  # Max 20 steps\n",
    "            action = agent.act(state, training=False)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            actions_taken.append(action_names[action])\n",
    "            path.append(next_state)\n",
    "            total_reward += reward\n",
    "            \n",
    "            print(f\"  Step {step + 1}: State {state} ‚Üí Action: {action_names[action]} ‚Üí State {next_state}\")\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                if reward > 0:\n",
    "                    print(f\"  üéâ SUCCESS! Reached goal in {step + 1} steps!\")\n",
    "                else:\n",
    "                    print(f\"  üíÄ Failed! Fell into hole at step {step + 1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"  üìä Total reward: {total_reward}\")\n",
    "        print(f\"  üõ§Ô∏è Path: {' ‚Üí '.join(map(str, path))}\")\n",
    "        print(f\"  üéÆ Actions: {' ‚Üí '.join(actions_taken)}\")\n",
    "\n",
    "demonstrate_agent(agent, env, num_demos=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40e6ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **üèóÔ∏è Built a Deep Q-Network** from scratch using PyTorch\n",
    "2. **üéØ Implemented key DQN features**: Experience replay, target networks, Œµ-greedy exploration\n",
    "3. **üèãÔ∏è Trained the agent** to solve the Frozen Lake environment\n",
    "4. **üìä Visualized the learning process** with comprehensive plots\n",
    "5. **üîç Analyzed the learned policy** through Q-value visualization\n",
    "6. **üéÆ Demonstrated the trained agent** in action\n",
    "\n",
    "- **Neural networks can learn optimal policies** even in stochastic environments\n",
    "- **Experience replay** helps stabilize learning by breaking correlations\n",
    "- **Target networks** provide stable learning targets\n",
    "- **Œµ-greedy exploration** balances exploration vs exploitation\n",
    "\n",
    "- **Traditional Q-Learning**: Uses a lookup table (16x4 = 64 entries for Frozen Lake)\n",
    "- **Deep Q-Learning**: Uses a neural network that can generalize and handle larger state spaces\n",
    "- **Advantage**: Better scalability and can learn complex patterns\n",
    "\n",
    "- Try different network architectures (deeper, wider, different activations)\n",
    "- Experiment with hyperparameters (learning rate, epsilon decay, etc.)\n",
    "- Test on larger environments (8x8 Frozen Lake)\n",
    "- Implement advanced techniques (Double DQN, Dueling DQN, Rainbow)\n",
    "\n",
    "You've successfully implemented and trained a Deep Q-Network to solve a classic reinforcement learning problem! The neural network learned to navigate the slippery frozen lake and reach the goal while avoiding holes.\n",
    "\n",
    "- [Deep Q-Learning Paper](https://arxiv.org/abs/1312.5602)\n",
    "- [Gymnasium Documentation](https://gymnasium.farama.org/)\n",
    "- [PyTorch RL Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
