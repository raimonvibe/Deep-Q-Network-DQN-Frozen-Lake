# ğŸ”ï¸ Deep Q-Network (DQN) for Frozen Lake Environment

![Frozen Lake Animation](lake.gif)

## ğŸ¯ Project Overview

Welcome to an exciting implementation of **Deep Q-Network (DQN)** applied to the classic **Frozen Lake** environment! ğŸ§Š This project demonstrates how deep reinforcement learning can solve navigation challenges in a slippery, icy world where one wrong step could lead to disaster! 

### ğŸŒŸ What Makes This Special?

- ğŸ§  **Deep Q-Learning**: Advanced neural network-based Q-learning implementation
- ğŸ® **Interactive Environment**: Navigate through a treacherous 4x4 frozen lake
- ğŸ“Š **Rich Visualizations**: Beautiful plots and animations of the learning process
- ğŸ¬ **Video Recording**: Watch your agent learn and master the environment
- âš¡ **GPU Optimized**: Designed to run efficiently on Google Colab T4 GPUs
- ğŸ“ˆ **Performance Tracking**: Comprehensive metrics and learning curves

## ğŸ—ºï¸ The Frozen Lake Challenge

The agent must navigate from the starting position (S) to the goal (G) while avoiding holes (H) on a slippery frozen surface:

```
S F F F
F H F H  
F F F H
H F F G
```

- **S**: Starting position ğŸ
- **F**: Frozen surface (safe) â„ï¸
- **H**: Hole (game over) ğŸ•³ï¸
- **G**: Goal (victory!) ğŸ¯

### ğŸ® Action Space
- **0**: Left â¬…ï¸
- **1**: Down â¬‡ï¸
- **2**: Right â¡ï¸
- **3**: Up â¬†ï¸

## ğŸš€ Quick Start on Google Colab T4

### ğŸ“‹ Prerequisites

1. **Google Colab Account**: Sign up at [colab.research.google.com](https://colab.research.google.com)
2. **T4 GPU Access**: Enable GPU runtime for optimal performance

### âš™ï¸ Setup Instructions

#### Step 1: Enable T4 GPU Runtime
1. Open Google Colab
2. Go to `Runtime` â†’ `Change runtime type`
3. Select `T4 GPU` as Hardware accelerator
4. Click `Save`

#### Step 2: Clone and Setup
```python
# Clone the repository
!git clone https://github.com/raimonvibe/Deep-Q-Network-DQN-Frozen-Lake.git
%cd Deep-Q-Network-DQN-Frozen-Lake

# Install required packages (automatically handled in the notebook)
!pip install gymnasium[toy-text] torch torchvision matplotlib seaborn numpy pandas tqdm imageio imageio-ffmpeg
```

#### Step 3: Run the Notebook
1. Upload the `frozen_lake_dqn_colab-final.ipynb` to your Colab environment
2. Run all cells sequentially
3. Watch your agent learn to navigate the frozen lake! ğŸ‰

### ğŸ”¥ Performance on T4 GPU
- **Training Time**: ~5-10 minutes for 2000 episodes
- **Memory Usage**: ~2GB GPU memory
- **Success Rate**: 90%+ after training
- **Convergence**: Typically within 1500 episodes

## ğŸ—ï¸ Project Architecture

### ğŸ“ File Structure
```
Deep-Q-Network-DQN-Frozen-Lake/
â”œâ”€â”€ ğŸ““ frozen_lake_dqn_colab-final.ipynb  # Main implementation notebook
â”œâ”€â”€ ğŸ¬ lake.gif                           # Environment animation
â”œâ”€â”€ ğŸ“„ LICENSE                            # MIT License
â””â”€â”€ ğŸ“– README.md                          # This file
```

### ğŸ§  Core Components

#### 1. **DQN Neural Network** ğŸ¤–
```python
class DQN(nn.Module):
    - Input: 16 states (4x4 grid positions)
    - Hidden: 128 neurons with ReLU activation
    - Output: 4 Q-values (one per action)
    - Dropout: 0.2 for regularization
```

#### 2. **Experience Replay Buffer** ğŸ’¾
```python
class ReplayBuffer:
    - Capacity: 10,000 experiences
    - Sampling: Random batch selection
    - Storage: (state, action, reward, next_state, done) tuples
```

#### 3. **DQN Agent** ğŸ¯
```python
class DQNAgent:
    - Epsilon-greedy exploration
    - Target network for stability
    - Adam optimizer (lr=0.001)
    - Gamma=0.99 for future rewards
```

## ğŸ“Š Key Features

### ğŸ¨ Visualizations
- **Environment Rendering**: Real-time grid visualization
- **Learning Curves**: Episode rewards and success rates
- **Q-Value Heatmaps**: Policy visualization
- **Training Progress**: Loss and epsilon decay

### ğŸ¬ Video Recording
- **Agent Demonstrations**: Watch successful episodes
- **Training Progress**: Record learning evolution
- **Success Compilation**: Highlight best performances

### ğŸ“ˆ Performance Metrics
- **Success Rate**: Percentage of episodes reaching the goal
- **Average Reward**: Mean reward per episode
- **Episode Length**: Steps taken to complete episodes
- **Q-Value Evolution**: Learning progress tracking

## ğŸ”§ Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 0.001 | Adam optimizer learning rate ğŸ“š |
| Gamma | 0.99 | Discount factor for future rewards ğŸ”® |
| Epsilon Start | 1.0 | Initial exploration rate ğŸ² |
| Epsilon Min | 0.01 | Minimum exploration rate ğŸ¯ |
| Epsilon Decay | 0.995 | Exploration decay rate ğŸ“‰ |
| Batch Size | 32 | Training batch size ğŸ“¦ |
| Buffer Size | 10,000 | Experience replay capacity ğŸ’¾ |
| Target Update | 100 | Target network update frequency ğŸ”„ |

## ğŸ“ Learning Algorithm

### ğŸ”„ Training Loop
1. **Initialize** environment and agent
2. **Observe** current state
3. **Select** action using Îµ-greedy policy
4. **Execute** action and observe reward
5. **Store** experience in replay buffer
6. **Sample** batch from replay buffer
7. **Update** Q-network using Bellman equation
8. **Update** target network periodically
9. **Decay** exploration rate
10. **Repeat** until convergence

### ğŸ“ Bellman Equation
```
Q(s,a) = r + Î³ * max(Q(s',a'))
```

Where:
- `Q(s,a)`: Q-value for state-action pair
- `r`: Immediate reward
- `Î³`: Discount factor
- `s'`: Next state
- `a'`: Next action

## ğŸ“ˆ Expected Results

After training, you should see:
- ğŸ¯ **Success Rate**: 85-95%
- ğŸ“Š **Average Reward**: 0.8-0.9
- â±ï¸ **Episode Length**: 6-8 steps (optimal path is 6)
- ğŸ§  **Convergence**: Around episode 1500

## ğŸ› ï¸ Troubleshooting

### Common Issues:

#### ğŸŒ Slow Training
- **Solution**: Ensure T4 GPU is enabled
- **Check**: `torch.cuda.is_available()` returns `True`

#### ğŸ“‰ Poor Performance
- **Solution**: Increase training episodes
- **Tip**: Adjust epsilon decay rate

#### ğŸ’¾ Memory Issues
- **Solution**: Reduce batch size or buffer size
- **Alternative**: Use CPU if GPU memory is limited

## ğŸ¤ Contributing

We welcome contributions! ğŸ‰

1. Fork the repository
2. Create a feature branch
3. Make your improvements
4. Add tests if applicable
5. Submit a pull request

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI Gymnasium**: For the Frozen Lake environment
- **PyTorch**: For the deep learning framework
- **Google Colab**: For providing free GPU access
- **Reinforcement Learning Community**: For inspiration and knowledge sharing

## ğŸ“š Further Reading

- ğŸ“– [Deep Q-Learning Paper](https://arxiv.org/abs/1312.5602)
- ğŸ“ [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)
- ğŸ§  [PyTorch RL Tutorials](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

---

**Happy Learning!** ğŸš€ May your agent master the frozen lake and inspire your next RL adventure! â„ï¸ğŸ¯

*Built with â¤ï¸ by [raimonvibe](https://github.com/raimonvibe)*
